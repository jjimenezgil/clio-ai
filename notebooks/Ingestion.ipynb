{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd32f1c8-0d0a-4f39-a62d-04bcc5a3f33f",
   "metadata": {},
   "source": [
    "# Data preparation and ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e4a621b-6c69-418a-b0bf-311088f5ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from statistics import median\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# from transformers import BigBirdModel, BigBirdTokenizer, LongformerTokenizer, LongformerModel\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a45e88-8705-4fca-b0bb-4bc5892a0aeb",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5aef7c64-1791-416f-bab9-0ef5ca5ab38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aeschylus</td>\n",
       "      <td>Agamemnon</td>\n",
       "      <td>lines 1-39</td>\n",
       "      <td>Release from this weary task of mine has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Ion</td>\n",
       "      <td>lines 1-40</td>\n",
       "      <td>Before the Temple of Apollo at Delphi . The su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Heracles</td>\n",
       "      <td>lines 1-25</td>\n",
       "      <td>Before the palace of Heracles at Thebes . Near...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Hippolytus</td>\n",
       "      <td>lines 1-33</td>\n",
       "      <td>Aphrodite enters above the skene. Aphrodite Ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Heracleidae</td>\n",
       "      <td>lines 1-47</td>\n",
       "      <td>The skene represents the temple of Zeus Agorai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author        title     section  \\\n",
       "0  Aeschylus    Agamemnon  lines 1-39   \n",
       "1  Euripides          Ion  lines 1-40   \n",
       "2  Euripides     Heracles  lines 1-25   \n",
       "3  Euripides   Hippolytus  lines 1-33   \n",
       "4  Euripides  Heracleidae  lines 1-47   \n",
       "\n",
       "                                                text  \n",
       "0  Release from this weary task of mine has been ...  \n",
       "1  Before the Temple of Apollo at Delphi . The su...  \n",
       "2  Before the palace of Heracles at Thebes . Near...  \n",
       "3  Aphrodite enters above the skene. Aphrodite Ap...  \n",
       "4  The skene represents the temple of Zeus Agorai...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Pandas df\n",
    "data_path = \"../data/ancient_sources.csv.gz\"\n",
    "df = pd.read_csv(data_path, compression=\"gzip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07f78d48-8bcd-4041-8b5f-5ecfa7960c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111709, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe66f1-2da3-4dfe-a2fc-cd60b0d89358",
   "metadata": {},
   "source": [
    "## Data cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13611913-2c27-4084-9b01-2393fbf6cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append index as ID\n",
    "df['Id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc6b9b01-f951-4672-be1e-71752a1651e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows without text\n",
    "df.drop(df[df[\"text\"].isna()].index, inplace=True)\n",
    "\n",
    "# Transform rows with NaN section to \"\"\n",
    "df['section'] = df['section'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b83d70a-72ea-4b6f-bb76-2c8549a9d868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111707, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check new size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d09d1d0b-7f5a-416c-bf8d-70819c4dd0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>Id</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aeschylus</td>\n",
       "      <td>Agamemnon</td>\n",
       "      <td>lines 1-39</td>\n",
       "      <td>Release from this weary task of mine has been ...</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Ion</td>\n",
       "      <td>lines 1-40</td>\n",
       "      <td>Before the Temple of Apollo at Delphi . The su...</td>\n",
       "      <td>1</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Heracles</td>\n",
       "      <td>lines 1-25</td>\n",
       "      <td>Before the palace of Heracles at Thebes . Near...</td>\n",
       "      <td>2</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Hippolytus</td>\n",
       "      <td>lines 1-33</td>\n",
       "      <td>Aphrodite enters above the skene. Aphrodite Ap...</td>\n",
       "      <td>3</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Heracleidae</td>\n",
       "      <td>lines 1-47</td>\n",
       "      <td>The skene represents the temple of Zeus Agorai...</td>\n",
       "      <td>4</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author        title     section  \\\n",
       "0  Aeschylus    Agamemnon  lines 1-39   \n",
       "1  Euripides          Ion  lines 1-40   \n",
       "2  Euripides     Heracles  lines 1-25   \n",
       "3  Euripides   Hippolytus  lines 1-33   \n",
       "4  Euripides  Heracleidae  lines 1-47   \n",
       "\n",
       "                                                text  Id  num_words  \n",
       "0  Release from this weary task of mine has been ...   0        445  \n",
       "1  Before the Temple of Apollo at Delphi . The su...   1        357  \n",
       "2  Before the palace of Heracles at Thebes . Near...   2        279  \n",
       "3  Aphrodite enters above the skene. Aphrodite Ap...   3        349  \n",
       "4  The skene represents the temple of Zeus Agorai...   4        554  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function that counts number of words of a text\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Append a column with the number of words of the texts\n",
    "df[\"num_words\"] = df[\"text\"].apply(count_words)\n",
    "\n",
    "# Show first rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "084e26e2-6dbb-45d9-91db-f212327fa7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    111707.000000\n",
       "mean        179.352252\n",
       "std         367.455390\n",
       "min           1.000000\n",
       "25%          58.000000\n",
       "50%          94.000000\n",
       "75%         173.000000\n",
       "max       31177.000000\n",
       "Name: num_words, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"num_words\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3390e5b-7d30-49b9-9c71-1e13412e1578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGzCAYAAADNKAZOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCQklEQVR4nO3dfVxVZb7///dmKwjqxhtEMFFRHEuhPGkaJCbliEZNDEPjaHWsqZzSpuNNZjildk5Jt1MzaTdO50Sn87UbjTwnTMvxlkaypCzxbsxAbeRGTdmkCLr39fuj317jFixRZMfi9Xw89mNY6/rsta61adxv1rqutRzGGCMAAACbCQp0BwAAAC4EQg4AALAlQg4AALAlQg4AALAlQg4AALAlQg4AALAlQg4AALAlQg4AALAlQg4AALAlQg5gM2vXrpXD4dCSJUsC3ZWzUl5erszMTHXu3FkOh0PPPfdcoLt01kaMGKERI0YEuhsAzoCQA5yDnJwcORwOtWnTRv/4xz/qtI8YMULx8fEB6FnzM3XqVH3wwQfKysrS66+/rtGjRwe6Sy3Stm3bNHfuXJWUlFzwfc2bN09Lly694PsBCDnAeaipqdHjjz8e6G40a6tXr9aNN96o+++/X7fccosuvvjiQHepRdq2bZseeeQRQg5shZADnIeBAwfqL3/5i/bv3x/orjS5o0ePNsp2Kioq1KFDh0bZVmNrrGP8qbDb8QA/hpADnIdZs2bJ4/H86NmckpISORwO5eTk1GlzOByaO3eutTx37lw5HA79/e9/1y233KLw8HB16dJFDz/8sIwx2rdvn2688Ua5XC5FRUXpmWeeqXefHo9Hs2bNUlRUlNq2batf/OIX2rdvX526jRs3avTo0QoPD1dYWJiuvvpq/e1vf/Or8fVp27ZtGj9+vDp27Khhw4b94DF//fXXuummm9SpUyeFhYXpyiuv1LJly6x23yU/Y4wWLFggh8Mhh8Nxxu1dfvnlysjI8FuXkJAgh8OhL7/80lr31ltvyeFwaPv27da6zz//XGPGjJHL5VK7du107bXX6uOPP/bblq8/69at06RJkxQZGanu3btb7QsXLlSfPn0UGhqqIUOGKD8/v95+Pv/88xowYIDCwsLUsWNHDR48WIsWLfrBz8o3juqtt94KyO8sJydHN910kyQpJSXF+l2sXbvWqlm+fLmSk5PVtm1btW/fXmlpadq6davVvnr1agUFBWn27Nl+2160aJEcDodefPFFSd//93706FG99tpr1n5uu+22H/x8gHNFyAHOQ2xsrP71X//1gpzNGTt2rLxerx5//HENHTpUjz76qJ577jn9/Oc/10UXXaQnnnhCcXFxuv/++7V+/fo673/ssce0bNkyzZw5U/fdd59WrlypkSNHqrq62qpZvXq1hg8fLrfbrTlz5mjevHk6cuSIrrnmGn3yySd1tnnTTTfp2LFjmjdvnu66664z9r28vFxJSUn64IMPNGnSJD322GM6fvy4fvGLX+jdd9+VJA0fPlyvv/66JOnnP/+5Xn/9dWu5PsnJyfroo4+s5W+//VZbt25VUFCQX+DIz89Xly5ddMkll0iStm7dquTkZH3xxRd64IEH9PDDD6u4uFgjRozQxo0b6+xn0qRJ2rZtm2bPnq0HH3xQkvSf//mf+t3vfqeoqCg9+eSTuuqqq+oNIH/5y1903333qX///nruuef0yCOPaODAgfXupz6B+p0NHz5c9913n6Tvg7vvd+H7DF9//XWlpaWpXbt2euKJJ/Twww9r27ZtGjZsmHV565prrtGkSZOUnZ2tzz77TJJUWlqq3//+9xo5cqTuvvtua1shISFKTk629vO73/3urD4foMEMgAZ79dVXjSTz6aefmt27d5tWrVqZ++67z2q/+uqrzYABA6zl4uJiI8m8+uqrdbYlycyZM8danjNnjpFkJk6caK07efKk6d69u3E4HObxxx+31h8+fNiEhoaaCRMmWOvWrFljJJmLLrrIuN1ua/3bb79tJJk//elPxhhjvF6v6du3r0lNTTVer9eqO3bsmImNjTU///nP6/Rp3LhxZ/X5TJkyxUgy+fn51rqqqioTGxtrevXqZTwej9/xT548+Ue3uXjxYiPJbNu2zRhjzP/93/+ZkJAQ84tf/MKMHTvWqrv00kvNL3/5S2s5PT3dBAcHm927d1vr9u/fb9q3b2+GDx9urfP9TocNG2ZOnjxpra+trTWRkZFm4MCBpqamxlq/cOFCI8lcffXV1robb7zR7/d+tn4KvzPf57tmzRq/9VVVVaZDhw7mrrvu8ltfVlZmwsPD/dYfPXrUxMXFmQEDBpjjx4+btLQ043K5zJ49e/ze27ZtW7//ZoELhTM5wHnq3bu3br31Vi1cuFClpaWNtt0777zT+tnpdGrw4MEyxuiOO+6w1nfo0EH9+vXT119/Xef9//qv/6r27dtby5mZmYqOjtb7778vSdq8ebN27dql8ePH69ChQzp48KAOHjyoo0eP6tprr9X69evl9Xr9tun7a/zHvP/++xoyZIjf5ZF27dpp4sSJKikp0bZt287uQzhFcnKyJFlnrfLz83XFFVfo5z//uXUm58iRIyoqKrJqPR6PPvzwQ6Wnp6t3797WtqKjozV+/Hh99NFHcrvdfvu566675HQ6reVNmzapoqJCd999t4KDg631t912m8LDw/3e26FDB33zzTf69NNPG3x8UmB/Z2eycuVKHTlyROPGjbP2d/DgQTmdTg0dOlRr1qyxasPCwpSTk6Pt27dr+PDhWrZsmZ599ln16NHjvPoAnCtCDtAIHnroIZ08ebJRZ1qd/sUQHh6uNm3aKCIios76w4cP13l/3759/ZYdDofi4uKsywu7du2SJE2YMEFdunTxe73yyiuqqalRZWWl3zZiY2PPqu979uxRv3796qz3Xf7Ys2fPWW3nVF27dlXfvn2tQJOfn6/k5GQNHz5c+/fv19dff62//e1v8nq9Vsg5cOCAjh07dsa+eL3eOpecTj9GX19P/zxbt27tF5wkaebMmWrXrp2GDBmivn37avLkyXXGyvyQQP7OzsS3z2uuuabOPj/88ENVVFT41V911VW655579Mknnyg1NVW//e1vz2v/wPloFegOAHbQu3dv3XLLLVq4cKE1juNUZxpQ6/F4zrjNU88m/NA6STLGnGVP/8n3F/9TTz2lgQMH1lvTrl07v+XQ0NAG76cxDRs2TKtWrVJ1dbUKCws1e/ZsxcfHq0OHDsrPz9f27dvVrl07/cu//Ms57+N8jvGSSy7Rzp07lZeXpxUrVuidd97RCy+8oNmzZ+uRRx455+36BOJ35tvn66+/rqioqDrtrVr5f43U1NRYA5Z3796tY8eOKSws7Lz6AJwrQg7QSB566CH9z//8j5544ok6bR07dpT0/eWUU53LGY2z5fsL3McYo6+++kqXXnqpJKlPnz6SJJfLpZEjRzbqvnv27KmdO3fWWb9jxw6r/VwkJyfr1Vdf1ZtvvimPx6OkpCQFBQVp2LBhVshJSkqywmCXLl0UFhZ2xr4EBQUpJibmR49F+v7zvOaaa6z1J06cUHFxsS677DK/+rZt22rs2LEaO3asamtrlZGRoccee0xZWVlq06bND+4rkL+zMwVx3z4jIyPPap9z5szR9u3b9fTTT2vmzJl68MEH9ec///ms9gU0Ni5XAY2kT58+uuWWW/Tyyy+rrKzMr83lcikiIqLOLKgXXnjhgvXnv//7v1VVVWUtL1myRKWlpRozZowkadCgQerTp4+efvppfffdd3Xef+DAgXPe93XXXadPPvlEBQUF1rqjR49q4cKF6tWrl/r3739O2/VdhnriiSd06aWXWmNikpOTtWrVKm3atMmqkb4/8zVq1Cj97//+r99N7srLy7Vo0SINGzZMLpfrB/c5ePBgdenSRS+99JJqa2ut9Tk5OXVC66FDh/yWg4OD1b9/fxljdOLEiR89vkD+ztq2bSupbhBPTU2Vy+XSvHnz6j2GU/e5ceNGPf3005oyZYqmT5+uGTNmaP78+Vq3bl2dfZ2+H+BC4EwO0Ij+8Ic/6PXXX9fOnTs1YMAAv7Y777xTjz/+uO68804NHjxY69ev19///vcL1pdOnTpp2LBhuv3221VeXq7nnntOcXFx1jTioKAgvfLKKxozZowGDBig22+/XRdddJH+8Y9/aM2aNXK5XHrvvffOad8PPvig3njjDY0ZM0b33XefOnXqpNdee03FxcV65513FBR0bn9fxcXFKSoqSjt37tTvf/97a/3w4cM1c+ZMSfILOZL06KOPauXKlRo2bJgmTZqkVq1a6eWXX1ZNTY2efPLJH91n69at9eijj+p3v/udrrnmGo0dO1bFxcV69dVX64zJGTVqlKKionTVVVepa9eu2r59u+bPn6+0tDS/AcVnEsjf2cCBA+V0OvXEE0+osrJSISEhuuaaaxQZGakXX3xRt956qy6//HL95je/UZcuXbR3714tW7ZMV111lebPn6/jx49rwoQJ6tu3rx577DFJ0iOPPKL33ntPt99+u7Zs2WIFqUGDBumvf/2r/vjHP6pbt26KjY3V0KFDz6nfwA8K6NwuoJk6dQr56SZMmGAk1ZlKfOzYMXPHHXeY8PBw0759e/PrX//aVFRUnHEK+YEDB+pst23btnX2d/p0dd905DfeeMNkZWWZyMhIExoaatLS0upM5TXGmM8//9xkZGSYzp07m5CQENOzZ0/z61//2qxatepH+/RDdu/ebTIzM02HDh1MmzZtzJAhQ0xeXl6dOp3lFHKfm266yUgyb731lrWutrbWhIWFmeDgYFNdXV3nPZ999plJTU017dq1M2FhYSYlJcVs2LDBr+aHfqfGGPPCCy+Y2NhYExISYgYPHmzWr19vrr76ar8p5C+//LIZPny49Vn26dPHzJgxw1RWVv7gMf1Ufmd/+ctfTO/evY3T6awznXzNmjUmNTXVhIeHmzZt2pg+ffqY2267zWzatMkYY8zUqVON0+k0Gzdu9Nvmpk2bTKtWrcw999xjrduxY4cZPny4CQ0NNZKYTo4LxmHMOYxYBAA0mrVr1yolJUWLFy9WZmZmoLsD2AZjcgAAgC0RcgAAgC0RcgAAgC0xJgcAANgSZ3IAAIAtEXIAAIAtteibAXq9Xu3fv1/t27fnNuMAADQTxhhVVVWpW7duP3hz0RYdcvbv3/+jz60BAAA/Tfv27VP37t3P2N6iQ47vNuv79u370efXAACAnwa3262YmJgffVxKiw45vktULpeLkAMAQDPzY0NNGHgMAABsiZADAABsiZADAABsiZADAABsiZADAABsiZADAABsiZADAABsiZADAABsqUXfDBCAPXk8HuXn56u0tFTR0dFKTk6W0+kMdLcANDHO5ACwldzcXMXFxSklJUXjx49XSkqK4uLilJubG+iuAWhihBwAtpGbm6vMzEwlJCSooKBAVVVVKigoUEJCgjIzMwk6QAvjMMaYQHciUNxut8LDw1VZWcmzq4BmzuPxKC4uTgkJCVq6dKmCgv75N5zX61V6erqKioq0a9cuLl0BzdzZfn9zJgeALeTn56ukpESzZs3yCziSFBQUpKysLBUXFys/Pz9APQTQ1Ag5AGyhtLRUkhQfH19vu2+9rw6A/RFyANhCdHS0JKmoqKjedt96Xx0A+yPkALCF5ORk9erVS/PmzZPX6/Vr83q9ys7OVmxsrJKTkwPUQwBNjZADwBacTqeeeeYZ5eXlKT093W92VXp6uvLy8vT0008z6BhoQbgZIADbyMjI0JIlSzR9+nQlJSVZ62NjY7VkyRJlZGQEsHcAmhpTyJlCDtgOdzwG7I0p5AAAoEUj5ACwFR7rAMCHkAPANnisA4BTNSjkvPjii7r00kvlcrnkcrmUmJio5cuXW+3Hjx/X5MmT1blzZ7Vr106/+tWvVF5e7reNvXv3Ki0tTWFhYYqMjNSMGTN08uRJv5q1a9fq8ssvV0hIiOLi4pSTk1OnLwsWLFCvXr3Upk0bDR06VJ988klDDgWAzXg8Hk2fPl3XX3+9li5dqiuvvFLt2rXTlVdeqaVLl+r666/X/fffL4/HE+iuAmgiDQo53bt31+OPP67CwkJt2rRJ11xzjW688UZt3bpVkjR16lS99957Wrx4sdatW6f9+/f7zWbweDxKS0tTbW2tNmzYoNdee005OTmaPXu2VVNcXKy0tDSlpKRo8+bNmjJliu6880598MEHVs1bb72ladOmac6cOfrss8902WWXKTU1VRUVFef7eQBopnisA4A6zHnq2LGjeeWVV8yRI0dM69atzeLFi6227du3G0mmoKDAGGPM+++/b4KCgkxZWZlV8+KLLxqXy2VqamqMMcY88MADZsCAAX77GDt2rElNTbWWhwwZYiZPnmwtezwe061bN5Odnd2gvldWVhpJprKyskHvA/DTs2jRIiPJVFVV1dvudruNJLNo0aIm7hmAxna239/nPCbH4/HozTff1NGjR5WYmKjCwkKdOHFCI0eOtGouvvhi9ejRQwUFBZJkXRvv2rWrVZOamiq3222dDSooKPDbhq/Gt43a2loVFhb61QQFBWnkyJFWzZnU1NTI7Xb7vQDYA491AHC6BoecLVu2qF27dgoJCdHdd9+td999V/3791dZWZmCg4PVoUMHv/quXbuqrKxMklRWVuYXcHztvrYfqnG73aqurtbBgwfl8XjqrfFt40yys7MVHh5uvWJiYhp6+AB+onisA4DTNTjk9OvXT5s3b9bGjRt1zz33aMKECdq2bduF6Fujy8rKUmVlpfXat29foLsEoJHwWAcAp2vwYx2Cg4MVFxcnSRo0aJA+/fRT/elPf9LYsWNVW1urI0eO+J3NKS8vV1RUlCQpKiqqziwo3+yrU2tOn5FVXl4ul8ul0NBQOZ1OOZ3Oemt82ziTkJAQhYSENPSQATQTPNYBwKnO+z45Xq9XNTU1GjRokFq3bq1Vq1ZZbTt37tTevXuVmJgoSUpMTNSWLVv8ZkGtXLlSLpdL/fv3t2pO3YavxreN4OBgDRo0yK/G6/Vq1apVVg2AlisjI0NfffWV1qxZo0WLFmnNmjXatWsXAQdoiRoymvnBBx8069atM8XFxebLL780Dz74oHE4HObDDz80xhhz9913mx49epjVq1ebTZs2mcTERJOYmGi9/+TJkyY+Pt6MGjXKbN682axYscJ06dLFZGVlWTVff/21CQsLMzNmzDDbt283CxYsME6n06xYscKqefPNN01ISIjJyckx27ZtMxMnTjQdOnTwm7V1NphdBQBA83O2398NCjm//e1vTc+ePU1wcLDp0qWLufbaa62AY4wx1dXVZtKkSaZjx44mLCzM/PKXvzSlpaV+2ygpKTFjxowxoaGhJiIiwkyfPt2cOHHCr2bNmjVm4MCBJjg42PTu3du8+uqrdfry/PPPmx49epjg4GAzZMgQ8/HHHzfkUIwxhBwAAJqjs/3+5inkPIUcAIBmhaeQAwCAFo2QAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbImQAwAAbKlBISc7O1tXXHGF2rdvr8jISKWnp2vnzp1+NSNGjJDD4fB73X333X41e/fuVVpamsLCwhQZGakZM2bo5MmTfjVr167V5ZdfrpCQEMXFxSknJ6dOfxYsWKBevXqpTZs2Gjp0qD755JOGHA4AALCxBoWcdevWafLkyfr444+1cuVKnThxQqNGjdLRo0f96u666y6VlpZaryeffNJq83g8SktLU21trTZs2KDXXntNOTk5mj17tlVTXFystLQ0paSkaPPmzZoyZYruvPNOffDBB1bNW2+9pWnTpmnOnDn67LPPdNlllyk1NVUVFRXn+lkAAAAbcRhjzLm++cCBA4qMjNS6des0fPhwSd+fyRk4cKCee+65et+zfPlyXX/99dq/f7+6du0qSXrppZc0c+ZMHThwQMHBwZo5c6aWLVumoqIi632/+c1vdOTIEa1YsUKSNHToUF1xxRWaP3++JMnr9SomJka///3v9eCDD55V/91ut8LDw1VZWSmXy3WuHwMAAGhCZ/v9fV5jciorKyVJnTp18lv///7f/1NERITi4+OVlZWlY8eOWW0FBQVKSEiwAo4kpaamyu12a+vWrVbNyJEj/baZmpqqgoICSVJtba0KCwv9aoKCgjRy5Eirpj41NTVyu91+LwAAYE+tzvWNXq9XU6ZM0VVXXaX4+Hhr/fjx49WzZ09169ZNX375pWbOnKmdO3cqNzdXklRWVuYXcCRZy2VlZT9Y43a7VV1drcOHD8vj8dRbs2PHjjP2OTs7W4888si5HjIAAGhGzjnkTJ48WUVFRfroo4/81k+cONH6OSEhQdHR0br22mu1e/du9enT59x72giysrI0bdo0a9ntdismJiaAPQIAABfKOYWce++9V3l5eVq/fr26d+/+g7VDhw6VJH311Vfq06ePoqKi6syCKi8vlyRFRUVZ/+tbd2qNy+VSaGionE6nnE5nvTW+bdQnJCREISEhZ3eQAACgWWvQmBxjjO699169++67Wr16tWJjY3/0PZs3b5YkRUdHS5ISExO1ZcsWv1lQK1eulMvlUv/+/a2aVatW+W1n5cqVSkxMlCQFBwdr0KBBfjVer1erVq2yagAAQMvWoDM5kydP1qJFi/S///u/at++vTWGJjw8XKGhodq9e7cWLVqk6667Tp07d9aXX36pqVOnavjw4br00kslSaNGjVL//v1166236sknn1RZWZkeeughTZ482TrLcvfdd2v+/Pl64IEH9Nvf/larV6/W22+/rWXLlll9mTZtmiZMmKDBgwdryJAheu6553T06FHdfvvtjfXZAACA5sw0gKR6X6+++qoxxpi9e/ea4cOHm06dOpmQkBATFxdnZsyYYSorK/22U1JSYsaMGWNCQ0NNRESEmT59ujlx4oRfzZo1a8zAgQNNcHCw6d27t7WPUz3//POmR48eJjg42AwZMsR8/PHHDTkcU1lZaSTV6R8AAPjpOtvv7/O6T05zx31yAABofprkPjkAAAA/VYQcAABgS+d8nxwA+KnyeDzKz89XaWmpoqOjlZycLKfTGehuAWhinMkBYCu5ubmKi4tTSkqKxo8fr5SUFMXFxVl3XQfQchByANhGbm6uMjMzlZCQoIKCAlVVVVnPy8vMzCToAC0Ms6uYXQXYgsfjUVxcnBISErR06VIFBf3zbziv16v09HQVFRVp165dXLoCmjlmVwFoUfLz81VSUqJZs2b5BRxJCgoKUlZWloqLi5Wfnx+gHgJoaoQcALZQWloqSYqPj6+33bfeVwfA/gg5AGzB93y8oqKiett96311AOyPkAPAFpKTk9WrVy/NmzdPXq/Xr83r9So7O1uxsbFKTk4OUA8BNDVCDgBbcDqdeuaZZ5SXl6f09HS/2VXp6enKy8vT008/zaBjoAXhZoAAbCMjI0NLlizR9OnTlZSUZK2PjY3VkiVLlJGREcDeAWhqTCFnCjlgO9zxGLC3s/3+5kwOANtxOp0aMWJEoLsBIMAYkwMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGyJkAMAAGypVaA7AACNzePxKD8/X6WlpYqOjlZycrKcTmeguwWgiXEmB4Ct5ObmKi4uTikpKRo/frxSUlIUFxen3NzcQHcNQBMj5ACwjdzcXGVmZiohIUEFBQWqqqpSQUGBEhISlJmZSdABWhiHMcYEuhOB4na7FR4ersrKSrlcrkB3B8B58Hg8iouLU0JCgpYuXaqgoH/+Def1epWenq6ioiLt2rWLS1dAM3e239+cyQFgC/n5+SopKdGsWbP8Ao4kBQUFKSsrS8XFxcrPzw9QDwE0NUIOAFsoLS2VJMXHx9fb7lvvqwNgfw0KOdnZ2briiivUvn17RUZGKj09XTt37vSrOX78uCZPnqzOnTurXbt2+tWvfqXy8nK/mr179yotLU1hYWGKjIzUjBkzdPLkSb+atWvX6vLLL1dISIji4uKUk5NTpz8LFixQr1691KZNGw0dOlSffPJJQw4HgI1ER0dLkoqKiupt96331QGwvwaFnHXr1mny5Mn6+OOPtXLlSp04cUKjRo3S0aNHrZqpU6fqvffe0+LFi7Vu3Trt379fGRkZVrvH41FaWppqa2u1YcMGvfbaa8rJydHs2bOtmuLiYqWlpSklJUWbN2/WlClTdOedd+qDDz6wat566y1NmzZNc+bM0WeffabLLrtMqampqqioOJ/PA0AzlZycrF69emnevHnyer1+bV6vV9nZ2YqNjVVycnKAegigyZnzUFFRYSSZdevWGWOMOXLkiGndurVZvHixVbN9+3YjyRQUFBhjjHn//fdNUFCQKSsrs2pefPFF43K5TE1NjTHGmAceeMAMGDDAb19jx441qamp1vKQIUPM5MmTrWWPx2O6detmsrOzz7r/lZWVRpKprKxswFED+Kl65513jMPhMDfccIPZsGGDcbvdZsOGDeaGG24wDofDvPPOO4HuIoBGcLbf3+c1JqeyslKS1KlTJ0lSYWGhTpw4oZEjR1o1F198sXr06KGCggJJsqZzdu3a1apJTU2V2+3W1q1brZpTt+Gr8W2jtrZWhYWFfjVBQUEaOXKkVVOfmpoaud1uvxcA+8jIyNCSJUu0ZcsWJSUlyeVyKSkpSUVFRVqyZInfWWUA9nfOdzz2er2aMmWKrrrqKmtAX1lZmYKDg9WhQwe/2q5du6qsrMyqOTXg+Np9bT9U43a7VV1drcOHD8vj8dRbs2PHjjP2OTs7W4888kjDDxZAs5GRkaEbb7yROx4DOPeQM3nyZBUVFemjjz5qzP5cUFlZWZo2bZq17Ha7FRMTE8AeAbgQnE6nRowYEehuAAiwcwo59957r/Ly8rR+/Xp1797dWh8VFaXa2lodOXLE72xOeXm5oqKirJrTZ0H5Zl+dWnP6jKzy8nK5XC6FhobK6XTK6XTWW+PbRn1CQkIUEhLS8AMGAADNToPG5BhjdO+99+rdd9/V6tWrFRsb69c+aNAgtW7dWqtWrbLW7dy5U3v37lViYqIkKTExUVu2bPGbBbVy5Uq5XC7179/fqjl1G74a3zaCg4M1aNAgvxqv16tVq1ZZNQAAoIVryGjme+65x4SHh5u1a9ea0tJS63Xs2DGr5u677zY9evQwq1evNps2bTKJiYkmMTHRaj958qSJj483o0aNMps3bzYrVqwwXbp0MVlZWVbN119/bcLCwsyMGTPM9u3bzYIFC4zT6TQrVqywat58800TEhJicnJyzLZt28zEiRNNhw4d/GZt/RhmVwEA0Pyc7fd3g0KOpHpfr776qlVTXV1tJk2aZDp27GjCwsLML3/5S1NaWuq3nZKSEjNmzBgTGhpqIiIizPTp082JEyf8atasWWMGDhxogoODTe/evf324fP888+bHj16mODgYDNkyBDz8ccfN+RwCDkAADRDZ/v9zQM6eUAnAADNCg/oBAAALRohBwAA2BIhBwAA2BIhBwAA2NI53/EYAH6qPB4Pj3UAwJkcAPaSm5uruLg4paSkaPz48UpJSVFcXJxyc3MD3TUATYyQA8A2cnNzlZmZqYSEBBUUFKiqqkoFBQVKSEhQZmYmQQdoYbhPDvfJAWzB4/EoLi5OCQkJWrp0qYKC/vk3nNfrVXp6uoqKirRr1y4uXQHNHPfJAdCi5Ofnq6SkRLNmzfILOJIUFBSkrKwsFRcXKz8/P0A9BNDUCDkAbKG0tFSSFB8fX2+7b72vDoD9EXIA2EJ0dLQkqaioqN5233pfHQD7I+QAsIXk5GT16tVL8+bNk9fr9Wvzer3Kzs5WbGyskpOTA9RDAE2NkAPAFpxOp5555hnl5eUpPT3db3ZVenq68vLy9PTTTzPoGGhBuBkgANvIyMjQkiVLNH36dCUlJVnrY2NjtWTJEmVkZASwdwCaGlPImUIO2A53PAbs7Wy/vzmTA8B2nE6nRowYEehuAAgwxuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbIuQAAABbahXoDgBAY/N4PMrPz1dpaamio6OVnJwsp9MZ6G4BaGKcyQFgK7m5uYqLi1NKSorGjx+vlJQUxcXFKTc3N9BdA9DEOJMDwDZyc3OVmZmptLQ0zZgxQ6Ghoaqurtby5cuVmZmpJUuWKCMjI9DdBNBEHMYYE+hOBIrb7VZ4eLgqKyvlcrkC3R0A58Hj8SguLk4RERE6ePCgSkpKrLZevXopIiJChw4d0q5du7h0BTRzZ/v9zeUqALaQn5+vkpISFRYWKiEhQQUFBaqqqlJBQYESEhJUWFio4uJi5efnB7qrAJoIIQeALfzjH/+QJI0ePVpLly7VlVdeqXbt2unKK6/U0qVLNXr0aL86APZHyAFgCwcOHJAkZWRkKCjI/5+2oKAgpaen+9UBsD9CDgBb6NKli6TvBx97vV6/Nq/Xq6VLl/rVAbA/Qg4AW7joooskScuXL1d6errfmJz09HQtX77crw6A/TG7itlVgC2cOrvqwIED2rNnj9XG7CrAXs72+5v75ACwBafTqWeeeabe++SsWLFCy5Yt05IlSwg4QAtCyAFgGxkZGVqyZImmT5+uvLw8a31sbCw3AgRaIC5XcbkKsJ3a2lq98MIL2r17t/r06aNJkyYpODg40N0C0Egu2M0A169frxtuuEHdunWTw+GwZiz43HbbbXI4HH4v3/0pfL799lvdfPPNcrlc6tChg+644w599913fjVffvmlkpOT1aZNG8XExOjJJ5+s05fFixfr4osvVps2bZSQkKD333+/oYcDwGZyc3PVr18/TZ06VfPnz9fUqVPVr18/nl0FtEANDjlHjx7VZZddpgULFpyxZvTo0SotLbVeb7zxhl/7zTffrK1bt2rlypXKy8vT+vXrNXHiRKvd7XZr1KhR6tmzpwoLC/XUU09p7ty5WrhwoVWzYcMGjRs3TnfccYc+//xzpaenKz09XUVFRQ09JAA24Xt2VX13PM7MzCToAC3MeV2ucjgcevfdd62bbEnfn8k5cuRInTM8Ptu3b1f//v316aefavDgwZKkFStW6LrrrtM333yjbt266cUXX9Qf/vAHlZWVWaeYH3zwQS1dulQ7duyQJI0dO1ZHjx71u+5+5ZVXauDAgXrppZfOqv9crgLswze7KiEhQUuXLvW7IaDX67X+CGJ2FdD8BfTZVWvXrlVkZKT69eune+65R4cOHbLaCgoK1KFDByvgSNLIkSMVFBSkjRs3WjXDhw/3u4aempqqnTt36vDhw1bNyJEj/fabmpqqgoKCM/arpqZGbrfb7wXAHnzPrpo1a1a9dzzOysri2VVAC9PoIWf06NH67//+b61atUpPPPGE1q1bpzFjxsjj8UiSysrKFBkZ6feeVq1aqVOnTiorK7Nqunbt6lfjW/6xGl97fbKzsxUeHm69YmJizu9gAfxklJaWSpLi4+Prbfet99UBsL9GDzm/+c1v9Itf/EIJCQlKT09XXl6ePv30U61du7axd9VgWVlZqqystF779u0LdJcANJLo6GhJOuO4PN96Xx0A+7vgj3Xo3bu3IiIi9NVXX0mSoqKiVFFR4Vdz8uRJffvtt4qKirJqysvL/Wp8yz9W42uvT0hIiFwul98LgD0kJyerV69emjdvXr3PrsrOzlZsbKySk5MD1EMATe2C3wzwm2++0aFDh6y/nhITE3XkyBEVFhZq0KBBkqTVq1fL6/Vq6NChVs0f/vAHnThxQq1bt5YkrVy5Uv369VPHjh2tmlWrVmnKlCnWvlauXKnExMQLfUgAfoJOvePxjTfeqNGjR3PHY6ClMw1UVVVlPv/8c/P5558bSeaPf/yj+fzzz82ePXtMVVWVuf/++01BQYEpLi42f/3rX83ll19u+vbta44fP25tY/To0eZf/uVfzMaNG81HH31k+vbta8aNG2e1HzlyxHTt2tXceuutpqioyLz55psmLCzMvPzyy1bN3/72N9OqVSvz9NNPm+3bt5s5c+aY1q1bmy1btpz1sVRWVhpJprKysqEfA4CfqBkzZphWrVoZSdarVatWZsaMGYHuGoBGcrbf3w0OOWvWrPH7x8P3mjBhgjl27JgZNWqU6dKli2ndurXp2bOnueuuu0xZWZnfNg4dOmTGjRtn2rVrZ1wul7n99ttNVVWVX80XX3xhhg0bZkJCQsxFF11kHn/88Tp9efvtt83PfvYzExwcbAYMGGCWLVvWoGMh5AD28s477xiHw2Guv/56s2DBAvNf//VfZsGCBeb66683DofDvPPOO4HuIoBGcLbf3zzWgfvkALbAfXKAliOg98kBgKbGfXIAnI6QA8AWuE8OgNMRcgDYwqn3yfF4PFq7dq3eeOMNrV27Vh6Ph/vkAC0QY3IYkwPYgm9MTkREhA4ePKiSkhKrrVevXoqIiNChQ4cYkwPYAGNyALQoTqdTN910kzZt2qTq6motXLhQ+/fv18KFC1VdXa1NmzYpMzOTgAO0IJzJ4UwOYAunnsk5cOCA9uzZY7VxJgewl7P9/r7gdzwGgKbgm131xhtv6IorrlB+fr5KS0sVHR2t5ORkffLJJ0pKSlJ+fr5GjBgR6O4CaAKEHAC2cOrsKqfTWSfIMLsKaHkYkwPAFngKOYDTEXIA2AJPIQdwOkIOAFvwPYU8Ly9P6enpKigoUFVVlQoKCpSenq68vDw9/fTTDDoGWhDG5ACwjYyMDC1ZskTTp09XUlKStT42NlZLlixRRkZGAHsHoKkxhZwp5IDteDyeOrOrOIMD2AdTyAG0WPXNrgLQ8jAmBwAA2BIhBwAA2BIhBwAA2BIhBwAA2BIDjwHYDrOrAEicyQFgM7m5uYqLi1NKSorGjx+vlJQUxcXFKTc3N9BdA9DECDkAbCM3N1eZmZlKSEjwu+NxQkKCMjMzCTpAC8PNALkZIGALHo9HcXFxSkhI0NKlSxUU9M+/4bxer9LT01VUVKRdu3Zx6Qpo5s72+5szOQBsIT8/XyUlJZo1a5ZfwJGkoKAgZWVlqbi4WPn5+QHqIYCmxsBjALZQWloqSYqPj6934HF8fLxfHQD7I+QAsIXo6GhJ0vz58/Xyyy+rpKTEauvVq5cmTpzoVwfA/rhcBcAWkpOT1aVLF2VlZSk+Pt5v4HF8fLxmzZqlyMhIJScnB7qrAJoIIQeAbTgcDutnY4z1AtAyEXIA2EJ+fr4qKiqUnZ2toqIiJSUlyeVyKSkpSVu3btW8efNUUVHBwGOgBSHkALAF34DimJiYOmdvvF6vevTo4VcHwP4IOQBswTeg+JZbbtGll17qNybn0ksv1S233OJXB8D+uBkgNwMEbKG2tlZt27ZV586d9c0336hVq39OHj158qS6d++uQ4cO6ejRowoODg5gTwGcL24GCKBF2bBhg06ePKmKigplZGT4ncnJyMhQRUWFTp48qQ0bNgS6qwCaCCEHgC34xtq8/vrr2rJli9/A46KiIr3++ut+dQDsj5sBArAF31ibPn36aOfOnXrhhRe0e/du9enTR5MmTVJhYaFfHQD7Y0wOY3IAW/A9oDMiIkIHDx6sc8fjiIgIHTp0iAd0AjbAmBwALYrT6dRNN92kTZs2qbq6WgsXLtT+/fu1cOFCVVdXa9OmTcrMzCTgAC0IZ3I4kwPYwqlncg4cOKA9e/ZYbZzJAezlbL+/GZMDwBby8/NVUlKiN954Q1dccUWdp5B/8sknSkpKUn5+vkaMGBHo7gJoAlyuAmALvllT8fHx9bb71jO7Cmg5OJMDwBZ8s6bmz5+vl19+uc7A44kTJ/rVAbA/zuQAsIXk5GR16dJFWVlZio+P97sZYHx8vGbNmqXIyEglJycHuqsAmgghB4BtOBwO62djjPUC0DIRcgDYQn5+vioqKpSdnV3vHY/nzZuniooK5efnB7qrAJpIg0PO+vXrdcMNN6hbt25yOBxaunSpX7sxRrNnz1Z0dLRCQ0M1cuRI7dq1y6/m22+/1c033yyXy6UOHTrojjvu0HfffedX8+WXXyo5OVlt2rRRTEyMnnzyyTp9Wbx4sS6++GK1adNGCQkJev/99xt6OABswjegOCYmpk6bMUY9evTwqwNgfw0OOUePHtVll12mBQsW1Nv+5JNP6s9//rNeeuklbdy4UW3btlVqaqqOHz9u1dx8883aunWrVq5cqby8PK1fv94aFCh9P/991KhR6tmzpwoLC/XUU09p7ty5WrhwoVWzYcMGjRs3TnfccYc+//xzpaenKz09XUVFRQ09JAA24BtQfOutt6qiosKvraKiQrfeeqtfHYAWwJwHSebdd9+1lr1er4mKijJPPfWUte7IkSMmJCTEvPHGG8YYY7Zt22YkmU8//dSqWb58uXE4HOYf//iHMcaYF154wXTs2NHU1NRYNTNnzjT9+vWzln/961+btLQ0v/4MHTrU/O53vzvr/ldWVhpJprKy8qzfA+CnqaamxgQFBRlJJiQkxEiyXr7loKAgv39XADRPZ/v93ahjcoqLi1VWVqaRI0da68LDwzV06FAVFBRIkgoKCtShQwcNHjzYqhk5cqSCgoK0ceNGq2b48OEKDg62alJTU7Vz504dPnzYqjl1P74a337qU1NTI7fb7fcCYA/5+fnyer2SpBMnTvi1+Za9Xi9jcoAWpFFDTllZmSSpa9eufuu7du1qtZWVlSkyMtKvvVWrVurUqZNfTX3bOHUfZ6rxtdcnOztb4eHh1qu+a/cAmqfVq1dbP4eEhPi1nbp8ah0Ae2tRs6uysrJUWVlpvfbt2xfoLgFoJL5nVcXExCgiIsKvLSIiQt27d/erA2B/jXrH46ioKElSeXm53+C+8vJyDRw40Ko5fVDgyZMn9e2331rvj4qKUnl5uV+Nb/nHanzt9QkJCanzFx4AezD///1w9u3bpzZt2vi1HThwwJr8YLhvDtBiNOqZnNjYWEVFRWnVqlXWOrfbrY0bNyoxMVGSlJiYqCNHjqiwsNCqWb16tbxer4YOHWrVrF+/3u+6+sqVK9WvXz917NjRqjl1P74a334AtCy+KeKS5HK5NH36dC1YsEDTp0/3e0rxqXUA7K3BZ3K+++47ffXVV9ZycXGxNm/erE6dOqlHjx6aMmWKHn30UfXt21exsbF6+OGH1a1bN6Wnp0uSLrnkEo0ePVp33XWXXnrpJZ04cUL33nuvfvOb36hbt26SpPHjx+uRRx7RHXfcoZkzZ6qoqEh/+tOf9Oyzz1r7/bd/+zddffXVeuaZZ5SWlqY333xTmzZt8ptmDqDl6Ny5s/XzgQMH9Mwzz1jLp94J+dQ6ADbX0Glba9as8Zua6XtNmDDBGPP9NPKHH37YdO3a1YSEhJhrr73W7Ny5028bhw4dMuPGjTPt2rUzLpfL3H777aaqqsqv5osvvjDDhg0zISEh5qKLLjKPP/54nb68/fbb5mc/+5kJDg42AwYMMMuWLWvQsTCFHLCPWbNmWf8eORwOv3+ffFPLJZlZs2YFuqsAztPZfn87jGm5F6jdbrfCw8NVWVnpdzobQPPzhz/8QfPmzZP0/fi7mpoaq61NmzbWmJxZs2bpscceC0gfATSOs/3+blGzqwDYV6dOnSRJPXv2rPf2Ej179vSrA2B/jTq7CgACxTezcs+ePXVmV5WXl1tncn5oBiYAe+FMDgBbuOiii6yfa2tr/dpOnal5ah0AeyPkALCFpKQktWrVSuHh4daN/3y6d++u8PBwtWrVSklJSQHqIYCmRsgBYAsbNmzQyZMn5Xa7deDAAb+2iooKud1unTx5Uhs2bAhQDwE0NUIOAFsoLS2V9P0djX3jb3yOHz9u3enYVwfA/hh4DMAWTn3w73XXXafrrrtOoaGhqq6u1vvvv69ly5bVqQNgb4QcALbg9XolSR07dtTSpUvVqtU//3mbOHGiIiMjdfjwYasOgP1xuQqALaxfv16SdPjwYWVkZKigoEBVVVUqKChQRkaGDh8+7FcHwP4IOQBsZe7cudqyZYuSkpLkcrmUlJSkoqIizZkzJ9BdA9DECDkAbGHEiBGSpL/+9a/avn27nn32Wd1777169tlntW3bNq1atcqvDoD98ewqnl0F2ILH41F0dLQOHDhQ59lVvuXIyEjt379fTqczgD0FcL54dhWAFsXpdOq2226TJL+Ac+ryhAkTCDhAC8KZHM7kALbg8XgUFhZW55EOpwoODtaxY8cIOkAzx5kcAC3K8uXLrYBT32MdpO+fabV8+fIm7xuAwOA+OQBs4dTZUwMHDlRWVpZ1M8Dly5frm2++sequv/76QHUTQBMi5ACwBd99cHr27KmioiLl5eVZbb169VLPnj21Z88eqw6A/RFyANhCdHS0iouLtWfPHl133XW68cYbVV1drdDQUO3atUvvv/++VQegZSDkALCF3/72t9YTxn2B5kx1AFoGBh4DsIWqqqpGrQPQ/BFyANhCx44dG7UOQPPH5SoAtuC7VCVJo0aNUnV1tQ4dOqTOnTsrNDRUH374oVU3YcKEQHUTQBMi5ACwBV/IiYiI0I4dO7R3716rrWfPnoqIiNDBgwf9whAAeyPkALAF340ADx48qNDQUL+2iooKVVdX+9UBsD/G5ACwhZSUFOvn059Wc+ryqXUA7I1nV/HsKsAWvvvuO7Vv3/5H66qqqtSuXbsm6BGAC4VnVwFoUTZt2tSodQCaP0IOAFvYt2+fJKl169b1tvvW++oA2B8hB4AtbNy4UZJ0/fXXKyYmxq8tJiZGaWlpfnUA7I/ZVQBswTe88N1331WbNm382g4cOKClS5f61QGwP87kALCF3r17Wz+fPk381OVT6wDYGyEHgC3079/f+tnr9fq1nbp8ah0AeyPkALCFjz76qFHrADR/hBwAtlBcXNyodQCaP0IOAFsoLy+3fj59Gvmpy6fWAbA3Qg4AWzh+/Lj184kTJ/zaTl0+tQ6AvRFyANjC6Q/lPN86AM0fIQeALXTp0qVR6wA0f4QcALZw+rTx860D0PwRcgDYwoEDBxq1DkDzR8gBYAtnO2uK2VVAy0HIAWALR48etX4OCvL/p83pdNZbB8DeCDkAbMHhcFg/nz7uxuPx1FsHwN4aPeTMnTtXDofD73XxxRdb7cePH9fkyZPVuXNntWvXTr/61a/qnD7eu3ev0tLSFBYWpsjISM2YMUMnT570q1m7dq0uv/xyhYSEKC4uTjk5OY19KACakW7dujVqHYDm74KcyRkwYIBKS0ut16nPipk6daree+89LV68WOvWrdP+/fuVkZFhtXs8HqWlpam2tlYbNmzQa6+9ppycHM2ePduqKS4uVlpamlJSUrR582ZNmTJFd955pz744IMLcTgAmoHrr7++UesANH+tLshGW7VSVFRUnfWVlZX6z//8Ty1atEjXXHONJOnVV1/VJZdcoo8//lhXXnmlPvzwQ23btk1//etf1bVrVw0cOFD/8R//oZkzZ2ru3LkKDg7WSy+9pNjYWD3zzDOSpEsuuUQfffSRnn32WaWmpl6IQwLwE/fVV181ah2A5u+CnMnZtWuXunXrpt69e+vmm2/W3r17JUmFhYU6ceKERo4cadVefPHF6tGjhwoKCiRJBQUFSkhIUNeuXa2a1NRUud1ubd261ao5dRu+Gt82zqSmpkZut9vvBcAeVqxY0ah1AJq/Rg85Q4cOVU5OjlasWKEXX3xRxcXFSk5OVlVVlcrKyhQcHKwOHTr4vadr164qKyuTJJWVlfkFHF+7r+2Hatxut6qrq8/Yt+zsbIWHh1uvmJiY8z1cAD8Rvn8fGqsOQPPX6JerxowZY/186aWXaujQoerZs6fefvvtgD8zJisrS9OmTbOW3W43QQewCWNMo9YBaP4u+BTyDh066Gc/+5m++uorRUVFqba2VkeOHPGrKS8vt8bwREVF1Zlt5Vv+sRqXy/WDQSokJEQul8vvBcAeTr83zvnWAWj+Lvj/27/77jvt3r1b0dHRGjRokFq3bq1Vq1ZZ7Tt37tTevXuVmJgoSUpMTNSWLVtUUVFh1axcuVIul0v9+/e3ak7dhq/Gtw0ALQ8hB8DpGv3/7ffff7/WrVunkpISbdiwQb/85S/ldDo1btw4hYeH64477tC0adO0Zs0aFRYW6vbbb1diYqKuvPJKSdKoUaPUv39/3Xrrrfriiy/0wQcf6KGHHtLkyZMVEhIiSbr77rv19ddf64EHHtCOHTv0wgsv6O2339bUqVMb+3AANBOn30vrfOsANH+NPibnm2++0bhx43To0CF16dJFw4YN08cff6wuXbpIkp599lkFBQXpV7/6lWpqapSamqoXXnjBer/T6VReXp7uueceJSYmqm3btpowYYL+/d//3aqJjY3VsmXLNHXqVP3pT39S9+7d9corrzB9HGjBWrVqpRMnTpxVHYCWwWFa8Cg8t9ut8PBwVVZWMj4HaOaCg4PPKuS0bt1atbW1TdAjABfK2X5/c3EagC2c+nyqxqgD0PwRcgDYwukP5TzfOgDNHyEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYEiEHAADYUqtAdwAAfI4dO6YdO3Zc8P189tlnDX7PxRdfrLCwsAvQGwAXSrMPOQsWLNBTTz2lsrIyXXbZZXr++ec1ZMiQQHcLwDnYsWOHBg0adMH3cy77KCws1OWXX34BegPgQmnWIeett97StGnT9NJLL2no0KF67rnnlJqaqp07dyoyMjLQ3QNajK+/LFDNwT3nvZ2gmhote2XeOb33oYceOuvaRx99tMHbDyrfou2r/97g950uJKKnel+aeN7bAfDjHMYYE+hOnKuhQ4fqiiuu0Pz58yVJXq9XMTEx+v3vf68HH3ywTn1NTY1qamqsZbfbrZiYGFVWVsrlcjVZvwE7+eKLL/Tuvw3V3BEhge5KszB3bY1u/ssW9e3bN9BdAZott9ut8PDwH/3+brZncmpra1VYWKisrCxrXVBQkEaOHKmCgoJ635Odna1HHnmkqboItAiffvqpXi6s1f/tPBHorjQLpd8Z3RzoTgAtRLMNOQcPHpTH41HXrl391nft2vWMAxezsrI0bdo0a9l3JgfAuUtPT5fUOANzq6urVVJScl7buOWWW87Y9j//8z/nvN1evXopNDT0nN/v0759e87iAE2k2YaccxESEqKQEE6pA40pIiJCd955Z6Nt76qrrjqv9998881yOBx11jfjK/MAzlGzDTkRERFyOp0qLy/3W19eXq6oqKgA9QrATwGBBoDUjG8GGBwcrEGDBmnVqlXWOq/Xq1WrVikxkZkLAAC0dM32TI4kTZs2TRMmTNDgwYM1ZMgQPffcczp69Khuv/32QHcNAAAEWLMOOWPHjtWBAwc0e/ZslZWVaeDAgVqxYkWdwcgAAKDladb3yTlfZzvPHgAA/HSc7fd3sx2TAwAA8EMIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJaa9R2Pz5fvPohutzvAPQEAAGfL9739Y/czbtEhp6qqSpIUExMT4J4AAICGqqqqUnh4+BnbW/RjHbxer/bv36/27dvL4XAEujsAGpHb7VZMTIz27dvHY1sAmzHGqKqqSt26dVNQ0JlH3rTokAPAvng2HQAGHgMAAFsi5AAAAFsi5ACwpZCQEM2ZM0chISGB7gqAAGFMDgAAsCXO5AAAAFsi5AAAAFsi5AAAAFsi5AAAAFsi5AAAAFsi5ACwlfXr1+uGG25Qt27d5HA4tHTp0kB3CUCAEHIA2MrRo0d12WWXacGCBYHuCoAAa9FPIQdgP2PGjNGYMWMC3Q0APwGcyQEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALbE7CoAtvLdd9/pq6++spaLi4u1efNmderUST169AhgzwA0NYcxxgS6EwDQWNauXauUlJQ66ydMmKCcnJym7xCAgCHkAAAAW2JMDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsCVCDgAAsKX/D2OvPVyknuS+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Boxplot\n",
    "boxplot_stats = plt.boxplot(df['num_words'])\n",
    "plt.title('Number of words per text');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b160ea2a-974b-4629-b7d7-9156888af039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper whisker: 345.0\n"
     ]
    }
   ],
   "source": [
    "whiskers = [item.get_ydata() for item in boxplot_stats['whiskers']]\n",
    "upp_whisker = whiskers[1][1]\n",
    "print(\"Upper whisker:\", upp_whisker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d8c7fe2-983e-4c0e-9909-d4258f3b7ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7786, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"num_words\"] > 512].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c9035-96a5-40dc-80d8-b9851ee64997",
   "metadata": {},
   "source": [
    "The upper limit stablished by boxplot is 345 words, so let's cut all the texts with a number of words higher than 512 (a bit higher than the boxplot upper limit). With this change, more of the 90% of the text will remain with the full text. Hopefully, this will make the embbeding calculation ligther."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a55e9a4-1582-44df-9ae2-a05d5fec0678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>Id</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aeschylus</td>\n",
       "      <td>Agamemnon</td>\n",
       "      <td>lines 1-39</td>\n",
       "      <td>Release from this weary task of mine has been ...</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Ion</td>\n",
       "      <td>lines 1-40</td>\n",
       "      <td>Before the Temple of Apollo at Delphi . The su...</td>\n",
       "      <td>1</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Heracles</td>\n",
       "      <td>lines 1-25</td>\n",
       "      <td>Before the palace of Heracles at Thebes . Near...</td>\n",
       "      <td>2</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Hippolytus</td>\n",
       "      <td>lines 1-33</td>\n",
       "      <td>Aphrodite enters above the skene. Aphrodite Ap...</td>\n",
       "      <td>3</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Euripides</td>\n",
       "      <td>Heracleidae</td>\n",
       "      <td>lines 1-47</td>\n",
       "      <td>The skene represents the temple of Zeus Agorai...</td>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author        title     section  \\\n",
       "0  Aeschylus    Agamemnon  lines 1-39   \n",
       "1  Euripides          Ion  lines 1-40   \n",
       "2  Euripides     Heracles  lines 1-25   \n",
       "3  Euripides   Hippolytus  lines 1-33   \n",
       "4  Euripides  Heracleidae  lines 1-47   \n",
       "\n",
       "                                                text  Id  num_words  \n",
       "0  Release from this weary task of mine has been ...   0        445  \n",
       "1  Before the Temple of Apollo at Delphi . The su...   1        357  \n",
       "2  Before the palace of Heracles at Thebes . Near...   2        279  \n",
       "3  Aphrodite enters above the skene. Aphrodite Ap...   3        349  \n",
       "4  The skene represents the temple of Zeus Agorai...   4        512  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define function that truncates the longer texts\n",
    "def truncate_texts(text, max_num_words=512):\n",
    "    word_list = text.split()\n",
    "    if len(word_list) > max_num_words:\n",
    "        word_list = word_list[:max_num_words]\n",
    "\n",
    "    return \" \".join(word_list)\n",
    "\n",
    "# Apply to all the texts\n",
    "df[\"text\"] = df[\"text\"].apply(truncate_texts)\n",
    "\n",
    "# Count again the num of words\n",
    "df[\"num_words\"] = df[\"text\"].apply(count_words)\n",
    "\n",
    "# Check df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "531c4e52-7df6-421d-9676-7190ee2f40e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    111707.000000\n",
       "mean        148.231821\n",
       "std         143.268450\n",
       "min           1.000000\n",
       "25%          58.000000\n",
       "50%          94.000000\n",
       "75%         173.000000\n",
       "max         512.000000\n",
       "Name: num_words, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check statistics again\n",
    "df[\"num_words\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "146915e6-8705-49a3-8f3e-4da171e15990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop number of words column\n",
    "df.drop('num_words', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a63ca-6394-444d-93bb-9a46769d5948",
   "metadata": {},
   "source": [
    "## Generate embeddings for vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6b467-9157-4839-9c3f-399c6116a609",
   "metadata": {},
   "source": [
    "We are going to use *msmarco-roberta-base-ance-firstp* model, which is well suited for long passages of text and is fine-tunned for dot product similarity, which is usually preferred for long passages of text (our case). Also, it is optimized for ANCE (Approximate Nearest Neighbor Search). Later, we will have to take all this into account by configuring Elasticsearch k-NN to use dot product and Approximate Nearest Neighbor Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc619f8e-b6df-4fa3-b9f1-423114e3ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a list of dicts\n",
    "df_dict = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Generate list with all the texts\n",
    "texts_list = [source[\"text\"]for source in df_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "caa031f1-a233-4315-a00a-ca299afe8264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the msmarco-distilroberta-base-v3 model\n",
    "model = SentenceTransformer('msmarco-roberta-base-ance-firstp')\n",
    "\n",
    "# Load it for GPU exec\n",
    "# model = SentenceTransformer('msmarco-roberta-base-ance-firstp', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f1264cb-12aa-4746-8e5c-219bd76634bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3401c118f3a1430e9bc2a6e3a7dd370c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    514\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 517\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    519\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    831\u001b[0m )\n\u001b[0;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         output_attentions,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:410\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:346\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    338\u001b[0m         hidden_states,\n\u001b[1;32m    339\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m         output_attentions,\n\u001b[1;32m    345\u001b[0m     )\n\u001b[0;32m--> 346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:288\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 288\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    290\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(texts_list, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99cf59-27cf-4f56-9539-729e057b5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of saving and loading the list\n",
    "# To save the list to a file\n",
    "with open('../data/data_with_embbedings.pkl', 'wb') as f:\n",
    "    pickle.dump(df_dict, f)\n",
    "\n",
    "# To load the list back from the file\n",
    "with open('../data/data_with_embbedings.pkl', 'rb') as f:\n",
    "    loaded_list = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
